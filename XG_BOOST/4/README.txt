XGBoost (Extreme Gradient Boosting)
Documentation

tqchen github

dmlc github

â€œGradient Boostingâ€ is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman.

XGBoost is based on this original model.

Supervised Learning

Objective Function : Training Loss + Regularization
ğ‘‚ğ‘ğ‘—(Î˜)=ğ¿(Î¸)+Î©(Î˜)
 
ğ¿  is the training loss function, and
Î©  is the regularization term.
Training Loss
The training loss measures how predictive our model is on training data.

Example 1, Mean Squared Error for Linear Regression:

ğ¿(Î¸)=âˆ‘ğ‘–(ğ‘¦ğ‘–âˆ’ğ‘¦Ì‚ ğ‘–)2
 
Example 2, Logistic Loss for Logistic Regression:

ğ¿(Î¸)=âˆ‘ğ‘–[ğ‘¦ğ‘–ğ‘™ğ‘›(1+ğ‘’âˆ’ğ‘¦Ì‚ ğ‘–)+(1âˆ’ğ‘¦ğ‘–)ğ‘™ğ‘›(1+ğ‘’ğ‘¦Ì‚ ğ‘–)]
 
Regularization Term
The regularization term controls the complexity of the model, which helps us to avoid overfitting.


Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance.

For model, it might be more suitable to be called as regularized gradient boosting.


